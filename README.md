# Databricks_Spark_Streaming
Streaming using Databricks Spark

# Tutorial 1
In the following tutorial, a basic streaming utility is created in Databricks. It reads data from input files provided by Databricks for demo and create streaming event on top of it.
The tutorial can be accessed at the following link
https://databricks.com/spark/getting-started-with-apache-spark/streaming

The do-along notebook can be accessed from this github by the file names below:
- Stream.html (html file version of ipynb)
- Stream.ipynb (ipynb do along notebook)

# Tutorial 2
This is a more elaborate tutorial
The full video of the tutorial can be accessed at this link
https://databricks.com/session/writing-continuous-applications-with-structured-streaming-pyspark-api

The do-along Notebook and the presentation can be accessed below from this github:
- PySparkSAIS.dbc (after you import this .dbc file to Databricks, the following packaged items can be seen)
  - 1-continuous_application_sensor.ipynb
  - 2-continuous_application_ml_serving.ipynb
  - 3_continuous_event_time_agg_watermarking.ipynb
  - setup folder with datafiles
- PySparkStructuredStreaming_FINAL.pdf presentation
- Youtube link of the tutorial (it is same as in the databricks link above https://www.youtube.com/watch?v=c4MAgdriJzY&feature=youtu.be)


