{"cells":[{"cell_type":"code","source":["#Tutorial https://databricks.com/spark/getting-started-with-apache-spark/streaming"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":1},{"cell_type":"code","source":["#Each line in the file contains a JSON record with two fields: time and action.\n#{\"time\":1469501675,\"action\":\"Open\"}\n#{\"time\":1469501678,\"action\":\"Close\"}{\"time\":1469501680,\"action\":\"Open\"}{\"time\":1469501685,\"action\":\"Open\"}{\"time\":1469501686,\"action\":\"Open\"}{\"time\":1469501689,\"action\":\"Open\"}{\"time\":1469501691,\"action\":\"Open\"}{\"time\":1469501694,\"action\":\"Open\"}{\"time\":1469501696,\"action\":\"Close\"}{\"time\":1469501702,\"action\":\"Open\"}{\"time\":1469501703,\"action\":\"Open\"}{\"time\":1469501704,\"action\":\"Open\"}\n\n%fs ls /databricks-datasets/structured-streaming/events/"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">UsageError: Line magic function `%fs` not found.\n</div>"]}}],"execution_count":2},{"cell_type":"code","source":["from pyspark.sql.types import StructType, StructField, TimestampType, StringType\nfrom pyspark.sql.window import Window\nfrom pyspark.sql import window"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":3},{"cell_type":"code","source":["#Since the sample data is just a static set of files, you can emulate a stream from them by reading one file at a time, in the chronological order in which they were created.\ninputPath = '/databricks-datasets/structured-streaming/events/'"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":4},{"cell_type":"code","source":["# Define the schema to speed up processing\njsonSchema = StructType([StructField('time', TimestampType(), True), StructField('action', StringType(), True)])"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":5},{"cell_type":"code","source":["#Input Data Frame is created by using readStream on the jsonSchema from the input path where the files are present, reading one file at a time\nstreamingInputDF = (\n  spark\n    .readStream\n    .schema(jsonSchema)                     # Set the schema of the JSON data (same as above)\n    .option('maxFilesPerTrigger', 1)        # Treat a sequence of files as a stream by picking one file at a time\n    .json(inputPath)\n)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":6},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":7},{"cell_type":"code","source":["#Transformation is applied on streaming in Data by doing a count of action. As we know from previous steps, each file has two fields, timestamp and action\n#Action is like 'Open', 'Closed' etc, so from the following we will know how many opened, how many closed actions etc\nstreamingCountsDF = (\n  streamingInputDF\n      .groupBy(\n        streamingInputDF.action)\n      .count()\n)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":8},{"cell_type":"code","source":["#Start the Streaming job\nquery = (\n  streamingCountsDF           #Defined in previous step as groupby of count of action\n     .writeStream\n      .format('memory')       # memory = store in-memory table (for testing only)\n      .queryName('counts')    # counts = name of the in-memory table\n      .queryName('counts')    # complete = all the counts should be in the table\n      .outputMode('complete')\n      .start()\n)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":9},{"cell_type":"code","source":["#query is a handle to the streaming query named counts that is running in the background. This query continuously picks up files and updates the windowed counts.\n#The command window above reports the status of the stream and when we expand counts, we get a dashboard of the number of records processed, batch statistics, and the state of the aggregation as graphs above"],"metadata":{},"outputs":[],"execution_count":10},{"cell_type":"code","source":["#Interactively query the stream\n#We can periodically query the counts aggregation"],"metadata":{},"outputs":[],"execution_count":11},{"cell_type":"code","source":["%sql select action, count from counts"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>action</th><th>count</th></tr></thead><tbody><tr><td>Open</td><td>30510</td></tr><tr><td>Close</td><td>29490</td></tr></tbody></table></div>"]}}],"execution_count":12},{"cell_type":"code","source":["#The query changes result every time we execute it. It reflects the action count based on the input stream of data"],"metadata":{},"outputs":[],"execution_count":13},{"cell_type":"code","source":["%sql select action, count from counts"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>action</th><th>count</th></tr></thead><tbody><tr><td>Open</td><td>36513</td></tr><tr><td>Close</td><td>35487</td></tr></tbody></table></div>"]}}],"execution_count":14},{"cell_type":"code","source":["%sql select action, count from counts"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>action</th><th>count</th></tr></thead><tbody><tr><td>Open</td><td>40495</td></tr><tr><td>Close</td><td>39505</td></tr></tbody></table></div>"]}}],"execution_count":15},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":16}],"metadata":{"name":"Stream","notebookId":3592998673143480},"nbformat":4,"nbformat_minor":0}
